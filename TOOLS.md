# TOOLS.md - Local Notes

## Ollama Setup
- **Endpoint:** http://localhost:11434
- **Model:** deepseek-coder-v2:16b
- **Use case:** Default for all conversations (cheapest option)
- **How to use:** Request tasks directly; I'll route to Ollama via CLI backend for maximum cost savings
- **Escalate to Claude when:** You explicitly say "use Claude" or "switch to a smart model"

## Model Strategy (Hybrid)
1. **Ollama (deepseek-coder-v2:16b)** — default for ALL work by default (cost-free)
2. **Claude Haiku** — when you explicitly request (paid, costs money)
3. **Claude Sonnet** — when you explicitly request (paid, costs money)

I will ONLY use paid models (Claude) if you explicitly ask me to or approve it. Otherwise, Ollama is my default. You set the bar — just tell me when to escalate.

---

Add whatever helps you do your job. This is your cheat sheet.
